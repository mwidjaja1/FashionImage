{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analyzing Fashion MNIST Data\n",
    "\n",
    "## About the Data\n",
    "Fashon-MNIST (https://github.com/zalandoresearch/fashion-mnist) is an up and coming new dataset of Zalando's article images. There are 60,000 training samples and 10,000 test samples in the dataset. Each sample is a 28x28 grayscale image which are associated with one of ten clothing labels, which look like this:\n",
    "\n",
    "![FashonMNIST](https://github.com/zalandoresearch/fashion-mnist/raw/master/doc/img/fashion-mnist-sprite.png)\n",
    "\n",
    "The ten clothing labels are:\n",
    "0. T-Shirt/top\n",
    "1. Trouser\n",
    "2. Pullover\n",
    "3. Dress\n",
    "4. Coat\n",
    "5. Sandal\n",
    "6. Shirt\n",
    "7. Sneaker\n",
    "8. Bag\n",
    "9. Ankle Boot\n",
    "\n",
    "(although in the dataset, the labels are zero indexed so T-Shirt/Top was labeled as '0' and Ankle Boot was labeled as '9')\n",
    "\n",
    "\n",
    "#### Why Fashion MNIST instead of the MNIST dataset?\n",
    "This might sound a bit strange because most data scientists tend to use the original MNIST dataset which contains several handwritten number samples. However, I really wanted to go down this route because of one key reason in that MNIST is too simple. The primary reason is that the Fashion MNIST dataset is relatively easy to predict nowadays with the advancement of many machine learning and neural network models.\n",
    "\n",
    "Even though this study was created by the team that created Fashion-MNIST, I don't have a reason to doubt their experiment. In this study, they created many machine learning models, ingested both the MNIST dataset and the Fashion-MNIST dataset into those models, and compared accuracy (http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/). Generally speaking, almost all of the Fashion-MNIST scores were less accurate than MNIST, which would support the theory that MNIST has become too easy to predict.\n",
    "\n",
    "To also build on the rationale, there's also a current script on GitHubGist where someone was able to compare MNIST digits based on one pixel, which (I'd hope) many machine learning models would quickly pick up on if it found it (https://gist.github.com/dgrtwo/aaef94ecc6a60cd50322c0054cc04478)\n",
    "\n",
    "Ultimately, I didn't want a dataset which would tell me 'stick to a machine learning model' again. I wanted a dataset that was just complex enough so that it's at least conceivable that I'd need a neural network model. I also wanted a dataset that was large enough so that I could definitiely conclude if I was overtraining my model or not.\n",
    "\n",
    "\n",
    "----\n",
    "# The Goal\n",
    "I wanted to determine which model could best model the Fashion-MNIST dataset. I have assistance here because the Zalando Research team has already tested many machine learning models on this dataset, so now all I'd have to do was try various neural network approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Machine Learning Models\n",
    "As specified earlier, the Zalando Research Team already tested many machine learning models on this dataset and the top 8 models were:\n",
    "\n",
    "![MachineLearning](Jupyter/MachineLearning.jpg)\n",
    "\n",
    "My takeaways from this were:\n",
    "- SVC is generally the best performer, but takes a long amount of training time edging out at 1 hour minimum.\n",
    "- GradientBoost isn't worth the runtime pains.\n",
    "- RandomForest has a lot of promise with a small training time and reasonable accuracy, but it still peaks out at 0.879 accuracy. I suspect I can do better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "----\n",
    "# Technical Setup\n",
    "\n",
    "For this project, I used\n",
    "- Anaconda 5.0.0 which uses Python 3.6.3\n",
    "  - TensorFlow 1.1.0\n",
    "  - Keras 2.0.8\n",
    "  - Theano 0.9.0 (Do not assume I'm using Theano unless otherwise specified)\n",
    "- iMac running macOS High Sierra with a:\n",
    "  - 3.8GHz quadâ€‘core Intel Core i5\n",
    "  - [When a GPU was required] EVGA GeForce GTX 1050 2 GBs\n",
    "\n",
    "`Keras` is a high level Python Package which lets me build neural networks and use either `TensorFlow`, `Theano`, or `Microsoft's CNTK` as the computation engine. It gives me the opportunity to test my model against all those computation packages without rewriting my model for each model.\n",
    "\n",
    "I began this project using Theano 0.9.0, and unfortunately midway, Theano was announced that it'll be depricated. At that point, I've switched to TensorFlow. I've redone most of my studies for TensorFlow but there will be certain tests that will stick remain on Theano. I will note this when it happens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Non-Model Specific Code\n",
    "I first created a function to handle the **input arguments** I might pass into my script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7ffe6a67ec3f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mplot\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Github/FashionImage/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputLayer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMaxPool2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalization\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from argparse import ArgumentParser\n",
    "import numpy as np\n",
    "\n",
    "import models\n",
    "import params\n",
    "import plot\n",
    "\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "def parse_args(inargs=None):\n",
    "    \"\"\" Parses input arguments \"\"\"\n",
    "    parser = ArgumentParser(\"./loader.py\")\n",
    "    standard_path = os.path.dirname(os.path.realpath(__file__))\n",
    "\n",
    "    iargs = parser.add_argument_group('Input Files/Data')\n",
    "    iargs.add_argument('--csv_file',\n",
    "                       default=os.path.join(standard_path, 'data.csv'),\n",
    "                       help='Path to CSV File')\n",
    "    iargs.add_argument('--model', default='cnn',\n",
    "                       help='Select: cnn (default), rnn, neural')\n",
    "\n",
    "    oargs = parser.add_argument_group('Output Files/Data')\n",
    "    oargs.add_argument('--out',\n",
    "                       default=os.path.join(standard_path, 'Run'),\n",
    "                       help='Path to save output files')\n",
    "\n",
    "    if not inargs:\n",
    "        args = parser.parse_args()\n",
    "    else:\n",
    "        args = parser.parse_args(inargs)\n",
    "    return args\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I had to create a function to **re-shape my data** to the appropriate shape. I'm using the `channels_first` setting in Keras which means that the quantity of my samples will be the first dimension of the dataset. I also had to reshape this array to four dimensions so that my dimensionality is (Quantity of Pictures, Quantity of Colors (just 1 since this is grayscale), Pixels Width, and Pixels Height)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def flatten_data(args, x_train, x_test, y_train, y_test):\n",
    "    \"\"\" Flattens data into a one dimension Numpy Array\n",
    "    \"\"\"\n",
    "    x_train = x_train.astype('float32') / 255\n",
    "    x_test = x_test.astype('float32') / 255\n",
    "\n",
    "    if args.model != 'rnn':\n",
    "        x_train = x_train.reshape(x_train.shape[0], 1, 28, 28)\n",
    "        x_test = x_test.reshape(x_test.shape[0], 1, 28, 28)\n",
    "\n",
    "    y_train = np_utils.to_categorical(y_train, 28)\n",
    "    y_test = np_utils.to_categorical(y_test, 28)\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I intend to run my script so that it can loop between a variety of options for a specific parameter. Because of that, I'd like it to save the:\n",
    "* Confusion Matrix for each option it tests\n",
    "* Some plot to compare each option it tests\n",
    "\n",
    "I created two **plotting functions** to do that.\n",
    "\n",
    "_Note: When code is ran in the Jupyter Notebook, it will NOT use the coding function below, but rather it'll use plot.py in the original directory where this Notebook rescides. That file is replicated below for completeness._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def conf_matrix(y_test, y_test_predict, classes, title='Confusion Matrix',\n",
    "                out=None):\n",
    "    # Converts both output arrays into just one column based on the class\n",
    "    y_test_predict_class = y_test_predict.argmax(1)\n",
    "    y_test_class = y_test.argmax(1)\n",
    "\n",
    "    # Creates confusion matrix\n",
    "    cm_data = confusion_matrix(y_test_class, y_test_predict_class)\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    # Plots Confusion Matrix\n",
    "    plt.figure()\n",
    "    plt.imshow(cm_data, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title(title)\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.yticks(tick_marks, classes)\n",
    "    plt.ylabel('True Label')\n",
    "\n",
    "    # Plots data on chart\n",
    "    thresh = cm_data.max() / 2.\n",
    "    for i, j in itertools.product(range(cm_data.shape[0]), range(cm_data.shape[1])):\n",
    "        plt.text(j, i, format(cm_data[i, j], 'd'),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm_data[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Saves or Shows Plot\n",
    "    if out:\n",
    "        plt.savefig(out)\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "def dict_trends(data, xlabel='Variable', out=None):\n",
    "    \"\"\" Plots a dictionary's worth of trends \"\"\"\n",
    "    data_df = pd.DataFrame.from_dict(data, orient='index')\n",
    "    ax = data_df.plot()\n",
    "\n",
    "    # Sets Axes\n",
    "    ax.set_xlabel(xlabel)\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Modifying {}'.format(xlabel))\n",
    "\n",
    "    # Saves or Shows Plot\n",
    "    if out:\n",
    "        plt.savefig(out)\n",
    "    else:\n",
    "        plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my script looks between different parameters, I want it to always have a default parameter value so that if I don't specify anything, it'll use the proper default setting. To do that, I created a **Parameters Configuration File** as params.py, which is effectively a dictionary.\n",
    "\n",
    "_Note: Like plot.py, when this function is called, it'll use the version in params.py rather than the version in Jupyter Notebook. params.py is replicated below for completeness._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def standard():\n",
    "    params = {}\n",
    "\n",
    "    # Build Parameters\n",
    "    params['conv_filters'] = 32\n",
    "    params['nb_pool'] = 2\n",
    "    params['nb_conv'] = 2\n",
    "    params['optimizer'] = 'nadam'\n",
    "    params['loss'] = 'categorical_crossentropy'\n",
    "\n",
    "    # Fit Parameters\n",
    "    params['epoch'] = 8\n",
    "    params['dropout'] = 0.1\n",
    "    params['batch_size'] = 128\n",
    "\n",
    "    # Dense Activation\n",
    "    params['dense_1'] = 120\n",
    "    params['activate_1'] = 'relu'\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As my script tests different models, I'd presumably like to run test data through the models to see if the predictions seem accurate. Because this fit function is the universal same function across all models, I chose to make a unified fit function in models.py as seen below.\n",
    "\n",
    "_Note: Like all other things, as I use this function, it'll use models.py rather than the version seen below_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_neural(model_params, shape):\n",
    "    \"\"\" Builds basic neural network model \"\"\"\n",
    "    from keras.layers import Dense, Flatten, InputLayer\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(shape[1], shape[2], shape[3])))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "    model.add(Dense(28, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=model_params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I created a **Main Function** which will connect all of the aforementioned functions as well as the model functions to be. It will run each model three times with each variable permutation option, average the results from those runs, and store it in a dictionary for comparisions later.\n",
    "\n",
    "_Note: This function shows the final state of the Main Function after all expansion. I'll talk about specific additions to this function in the Neural Network sections below._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-35cd40f314ea>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatasets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfashion_mnist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mapplications\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdata_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mio_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconv_utils\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Globally-importable utils.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/utils/conv_utils.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoves\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;32melif\u001b[0m \u001b[0m_BACKEND\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'tensorflow'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstderr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Using TensorFlow backend.\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mtensorflow_backend\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Unknown backend: '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_BACKEND\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmoving_averages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_array_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontrol_flow_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from keras.datasets import fashion_mnist\n",
    "from keras.utils import np_utils\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def main(args):\n",
    "    # Loads CSV File\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train, y_train, x_test, y_test = flatten_data(args, x_train, x_test, y_train, y_test)\n",
    "\n",
    "    # Creates output directory\n",
    "    if not os.path.isdir(args.out):\n",
    "        os.makedirs(args.out)\n",
    "\n",
    "    # Creates range to loop filter between\n",
    "    change = 'activation'\n",
    "    range = ['softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'selu',\n",
    "             'elu', 'linear']\n",
    "    history_dict = {x: {'loss': 0.0, 'acc': 0.0} for x in range}\n",
    "\n",
    "    # Runs Model\n",
    "    for new in range:\n",
    "        history_dict[new] = {'loss': [], 'acc': []}\n",
    "        for loop in [1, 2, 3]:\n",
    "            print('Creating Model with the {} {}'.format(new, change))\n",
    "            model_params = params.standard()\n",
    "            model_params['activate_1'] = new\n",
    "\n",
    "            if args.model == 'rnn':\n",
    "                model = models.basic_rnn(model_params, x_train.shape)\n",
    "            elif args.model == 'neural':\n",
    "                model = models.basic_neural(model_params, x_train.shape)\n",
    "            else:\n",
    "                model = models.double_cnn(model_params, x_train.shape)\n",
    "\n",
    "            y_pred, metrics = models.fit_model(model, model_params, \n",
    "                                               x_train, y_train, x_test, y_test)\n",
    "\n",
    "            # Adds Data to Trends\n",
    "            history_dict[new]['loss'].append(metrics['loss'])\n",
    "            history_dict[new]['acc'].append(metrics['acc'])\n",
    "\n",
    "        # Calculates Average\n",
    "        history_dict[new]['loss'] = np.mean(history_dict[new]['loss'])\n",
    "        history_dict[new]['acc'] = np.mean(history_dict[new]['acc'])\n",
    "\n",
    "        # Plots Confusion Matrix\n",
    "        classes = {0: 'T-Shirt/top',\n",
    "                   1: 'Trouser',\n",
    "                   2: 'Pullover',\n",
    "                   3: 'Dress',\n",
    "                   4: 'Coat',\n",
    "                   5: 'Sandal',\n",
    "                   6: 'Shirt',\n",
    "                   7: 'Sneaker',\n",
    "                   8: 'Bag',\n",
    "                   9: 'Ankle boot'}\n",
    "        class_values = list(classes.values())\n",
    "        title = \"{} (Loss {} & Acc {})\".format(new, metrics['loss'], metrics['acc'])\n",
    "        conf_png = '{}/{}_{}.png'.format(args.out, new, change)\n",
    "        plot.conf_matrix(y_test, y_pred, class_values, out=conf_png, title=title)\n",
    "\n",
    "    # Plots Accuracy & Loss Trends\n",
    "    trends_png = '{}/{}.png'.format(args.out, change)\n",
    "    plot.dict_trends(history_dict, xlabel=change, out=trends_png)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_pred\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ARGS = parse_args()\n",
    "    x_train, y_train, x_test, y_test, y_pred = main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# About Neural Networks\n",
    "\n",
    "### Neural Network Principles\n",
    "The primary Neural Network Layer in Keras is the **Dense** layer. In the most simple sense, this layer takes in an input, performs some calculation on them (typically a matrix vector multiplication type function), and outputs the data in some different dimensionality. This calculation is typically referred to as an **Activation** Function.\n",
    "\n",
    "It's worth noting that when I first get my data, it's technically in four dimensions as: (Quantity of Image Samples, Colorscale, Width Pixels, and Height Pixels). In this case:\n",
    "- Colorscale is always 1 because this ia greyscale image\n",
    "- Each image is 28x28 samples.\n",
    "\n",
    "I cannot immediately feed these images into the `Dense` Layer with that dimensionality. So I first need to send it through a **Flatten** layer which flattens it into a two dimension array as: (Quantity of Image Samples, Width Pixels * Height Pixels).\n",
    "\n",
    "\n",
    "----\n",
    "# Building a Neural Network\n",
    "### Layers I'll Use\n",
    "I wanted to begin by creating a basic neural network with only two Neural Layers. \n",
    "- The first layer will be a `Dense` layer and I will cycle between various activation functions to find the ideal one. (This of course, will happen after a `Flatten` layer. I will use the nadam optimzier initially, although I'll probably test this in a second experiment.\n",
    "- The second layer will be a `Dense` layer and this will stay under the `Softmax` Activation Function.\n",
    "\n",
    "Softmax is a logarithmic function which assigns probabilities for each possible option so that all options add to 1. Because of this, Softmax is regarded as one of the best 'final' functions to classify results. \n",
    "\n",
    "Speaking of classification, there has been some research in foregoing this final fit function, and rather, sending this data to another machine learning model and having that do the classification instead. It's plausible that given the success of Random Forests earlier, that Random Forests would do a better job at classifying the data produced by the Neural Network, than the Neural Network itself.\n",
    "\n",
    "\n",
    "\n",
    "### First Experiment: Testing Activation Functions in a Neural Network\n",
    "This is the code for my first Neural Network model. It only has two Dense Layers and we will loop between these **activation functions**:\n",
    "* `softplus`\n",
    "* `softsign`\n",
    "* `relu`\n",
    "* `thanh`\n",
    "* `sigmoid`\n",
    "* `hard_sigmoid`\n",
    "* `selu`\n",
    "* `elu`\n",
    "* `linear`\n",
    "\n",
    "![Activation](Jupyter/Activation.tiff)\n",
    "\n",
    "For now, I'm using the `nadam` optimizer & `categorical_crossentropy` loss function. More about the loss function later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_neural(model_params, shape):\n",
    "    \"\"\" Builds basic neural network model \"\"\"\n",
    "    from keras.layers import Dense, Flatten, InputLayer\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(shape[1], shape[2], shape[3])))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "    model.add(Dense(28, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=model_params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main(args):\n",
    "    # Loads CSV File\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train, y_train, x_test, y_test = flatten_data(args, x_train, x_test, y_train, y_test)\n",
    "\n",
    "    # Creates output directory\n",
    "    if not os.path.isdir(args.out):\n",
    "        os.makedirs(args.out)\n",
    "\n",
    "    # Creates range to loop filter between\n",
    "    change = 'activation'\n",
    "    range = ['softplus', 'softsign', 'relu', 'tanh', 'sigmoid', 'hard_sigmoid', 'selu',\n",
    "             'elu', 'linear']\n",
    "    history_dict = {x: {'loss': 0.0, 'acc': 0.0} for x in range}\n",
    "\n",
    "    # Runs Model\n",
    "    for new in range:\n",
    "        history_dict[new] = {'loss': [], 'acc': []}\n",
    "        for loop in [1, 2, 3]:\n",
    "            print('Creating Model with the {} {}'.format(new, change))\n",
    "            model_params = params.standard()\n",
    "            model_params['activate_1'] = new\n",
    "\n",
    "            model = models.basic_neural(model_params, x_train.shape)\n",
    "\n",
    "            y_pred, metrics = models.fit_model(model, model_params, \n",
    "                                               x_train, y_train, x_test, y_test)\n",
    "\n",
    "            # Adds Data to Trends\n",
    "            history_dict[new]['loss'].append(metrics['loss'])\n",
    "            history_dict[new]['acc'].append(metrics['acc'])\n",
    "\n",
    "        # Calculates Average\n",
    "        history_dict[new]['loss'] = np.mean(history_dict[new]['loss'])\n",
    "        history_dict[new]['acc'] = np.mean(history_dict[new]['acc'])\n",
    "\n",
    "        # Plots Confusion Matrix\n",
    "        classes = {0: 'T-Shirt/top',\n",
    "                   1: 'Trouser',\n",
    "                   2: 'Pullover',\n",
    "                   3: 'Dress',\n",
    "                   4: 'Coat',\n",
    "                   5: 'Sandal',\n",
    "                   6: 'Shirt',\n",
    "                   7: 'Sneaker',\n",
    "                   8: 'Bag',\n",
    "                   9: 'Ankle boot'}\n",
    "        class_values = list(classes.values())\n",
    "        title = \"{} (Loss {} & Acc {})\".format(new, metrics['loss'], metrics['acc'])\n",
    "        conf_png = '{}/{}_{}.png'.format(args.out, new, change)\n",
    "        plot.conf_matrix(y_test, y_pred, class_values, out=conf_png, title=title)\n",
    "\n",
    "    # Plots Accuracy & Loss Trends\n",
    "    trends_png = '{}/{}.png'.format(args.out, change)\n",
    "    plot.dict_trends(history_dict, xlabel=change, out=trends_png)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_pred\n",
    "\n",
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are the results from the test. We are considering two metrics here: `Accuracy` and `Loss`.\n",
    "\n",
    "* `Accuracy`... speaks for itself. The higher the accuracy, the better.\n",
    "* `Loss`, from a high-level point of view, calculates if the model is over-training to the data. The lower the loss, the better.\n",
    "\n",
    "I'm using the `categorical_crossentropy` method to compute loss. This method, unlike many of the other options, works for categorical classification problems where multiple classes are possible, such as this problem.\n",
    "\n",
    "![Neural_Activation](TensorData/Neural_Activation/Activation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the activation functions, besides for Linear, appeared to perform well. While Sigmoid had the lowest loss, Relu & Softplus had the highest accuracy. \n",
    "\n",
    "I chose Sigmoid because it had reasonable accuracy to the other activation functions, but had notably lower loss. The confusion matrix for this is attached below.\n",
    "\n",
    "![Neural_Sigmoid](TensorData/Neural_Activation/sigmoid_activation.png)\n",
    "\n",
    "I chose to stick to the **Sigmoid** Activation Function for the Basic Neural Network. For reference, the Sigmoid Function typically looks like:\n",
    "\n",
    "![Wikipedia_Sigmoid](https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Experiment: Testing Optimizers\n",
    "I now wanted to test the **optimizers** used by the first Neural Network Layer across these options:\n",
    "\n",
    "- `RMSprop`\n",
    "- `Adagrad`\n",
    "- `Adadelta`\n",
    "- `Adam`\n",
    "- `Adamax`\n",
    "- `Nadam`\n",
    "\n",
    "To save on space, I won't replicate the code I used to run it, but it basically just involved me swapping out the 'range' and 'change' variables from main(). Below are the results _(disregard how the plot says 'epoch' rather than 'optimizer'. That was a formatting bug which did not affect the results)_:\n",
    "\n",
    "![Neural_Optimizer](TensorData/Neural_Optimizer/optimizer.png)\n",
    "\n",
    "The differences between each optimizer are marginal, but **Adam** optimizer had both the lowest loss & highest accuracy. Its Confusion Matrix is attached below:\n",
    "\n",
    "![Neural_Adagrad](TensorData/Neural_Optimizer/Adam_optimizer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Three (and the last for Neural Networks): Epochs\n",
    "An **Epoch** is a single pass of the data through the neural network model during the fitting process. Right now, I was using a default value of '8'. More Epochs usually increase accuracy, but it runs the risk of increasing loss & runtime (both are bad). \n",
    "\n",
    "Here are the runtime results for the epochs I tested for each pass of the model _(technically I run each model through an epoch setting three times, so I divide the elapsed run time by three for these results)_:\n",
    "* 1 Epoch: 1 Second\n",
    "* 4 Epochs: 4 Seconds\n",
    "* 8 Epochs: 8 Seconds\n",
    "* 12 Epochs: 12 Seconds\n",
    "* 16 Epochs: 17 Seconds\n",
    "* 20 Epochs: 21 Seconds\n",
    "* 24 Epochs: 25 Seconds\n",
    "\n",
    "And below are the actual metrics:\n",
    "![Neural_Epoch](TensorData/Neural_Epoch/epoch.png)\n",
    "\n",
    "We can see that at around 12-16 Epoch, we hit the highest accuracy before the loss begins increasing. The confusion matrix for **16 Epochs** is attached below, although any range between 12-16 seems to be optimal.\n",
    "![Neural_16Epoch](TensorData/Neural_Epoch/16_epoch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network Conclusions\n",
    "I ultimately got 0.328 Loss & 0.882 Accuracy using a two layer neural network, where the first layer used the Adam Optimizer and Sigmoid Activation Function and the second layer used the Softmax Activation Function. This appears to be relatively true at both 12 and 16 Epochs which takes 12 & 17 seconds to run each.\n",
    "\n",
    "Recall that for the Machine Learning Models:\n",
    "* SVC had the highest accuracy at 0.897 but needing 1:12 hours to run.\n",
    "* Random Forests had a decent accuracy at 0.879 but needing 8 minutes to run.\n",
    "\n",
    "Our model fell right in the middle with 0.882 accuracy, but only needed 12-17 seconds to run. This gives us the opportunity to add more Neural Network layers which would most likely increase accuracy.\n",
    "\n",
    "Another way we could increase accuracy is by investigating other types of Neural Networks (or rather, Neural Network Layers) and involving them in our mix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# About Convolutional Neural Networks\n",
    "\n",
    "### Convolutional Neural Network Principles\n",
    "_(All GIFs in this section are obtained from https://hackernoon.com/visualizing-parts-of-convolutional-neural-networks-using-keras-and-cats-5cc01b214e59)_\n",
    "\n",
    "A convolutional neural network does not eliminate the layers from the neural networks, but rather, augments it with its own layers. The CNN is primarily driven by the **Convolutional Layer,** which effectively is another way to simplify the data.\n",
    "\n",
    "![ConvolutionalLayer](https://cdn-images-1.medium.com/max/1600/1*ZCjPUFrB6eHPRi4eyP6aaA.gif)\n",
    "\n",
    "As seen in the image above,\n",
    "* The sliding yellow window is the _Kernel_\n",
    "* The _Stride_ of the kernel refers to how many 'pixels' it moves in each move\n",
    "* Each pixel has a _Filter_. A filter is a combination of weights (denoted in red text) and the weights change to accomodate what the CNN is learning. We multiply the weight to whatever value was originally in that square.\n",
    "\n",
    "This produces a _convolved feature_. There are further types of layers which we can do at this point to reduce the size of this convolved feature. One of those types is **Max Pooling** or **Average Pooling** in which we create a kernel on this convolved feature and completely move it to seperate regions, selecting either the single highest or the average value across all the values within that kernel.\n",
    "\n",
    "![Pooling](https://cdn-images-1.medium.com/max/800/1*Feiexqhmvh9xMGVVJweXhg.gif)\n",
    "\n",
    "We can also create **Dropout** Layers which will temporarily turn off certain outputs while training the model, to help reduce the risk that we're overfitting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# Building a CNN: Part 1\n",
    "\n",
    "### Four Parts\n",
    "To reiterate, the four permutations of layers I have initially are:\n",
    "\n",
    "1. Convolutional Layer\n",
    "2. Convolutional Layer + Max Pooling\n",
    "3. Convolutional Layer + Average Pooling\n",
    "4. Convolutional Layer + (whichever pooling wins) + Dropout\n",
    "\n",
    "I first wanted to begin with Convolutional Layer just to figure out my convolutional parameters & epochs before proceeding.\n",
    "\n",
    "Recall that in the Neural Network test, we determined that 12 epochs with a Dense Layer set to these values performed the best.\n",
    "* `Adam` Optimizer\n",
    "* `Sigmoid` Activation Function\n",
    "\n",
    "I'm going to carry these neural network settings into my CNN.\n",
    "\n",
    "### Components for the Convolutional Layer\n",
    "The convolutional layer has these parameters:\n",
    "* Number of Filters: Which is the number of layers requested in the output\n",
    "* A tuple containing the size of the kernel and the stride value\n",
    "* And something I'll keep static is the _padding_, which is how it'll ensure how each kernel doesn't go past the edge of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def basic_cnn(model_params, shape):\n",
    "    \"\"\" Builds basic Convolutional neural network model \"\"\"\n",
    "    from keras.layers import Dense, Dropout, Flatten, InputLayer, MaxPooling2D\n",
    "    from keras.layers.normalization import BatchNormalization\n",
    "    from keras.layers.convolutional import Conv2D\n",
    "    from keras.models import Sequential\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(InputLayer(input_shape=(shape[1], shape[2], shape[3])))\n",
    "    model.add(BatchNormalization())\n",
    "\n",
    "    model.add(Conv2D(model_params['conv_filters'],\n",
    "                     (model_params['nb_pool'], model_params['nb_conv']),\n",
    "                     padding='same'))\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(model_params['dense_1'], activation=model_params['activate_1']))\n",
    "    model.add(Dense(28, activation='softmax'))\n",
    "\n",
    "    model.compile(loss=model_params['loss'],\n",
    "                  optimizer=model_params['optimizer'],\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n",
    "\n",
    "def main(args):\n",
    "    # Loads CSV File\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train, y_train, x_test, y_test = flatten_data(args, x_train, x_test, y_train, y_test)\n",
    "\n",
    "    # Creates output directory\n",
    "    if not os.path.isdir(args.out):\n",
    "        os.makedirs(args.out)\n",
    "\n",
    "    # Creates range to loop filter between\n",
    "    change = 'conv_filters'\n",
    "    range = [4, 14, 24, 32]\n",
    "    history_dict = {x: {'loss': 0.0, 'acc': 0.0} for x in range}\n",
    "\n",
    "    # Runs Model\n",
    "    for new in range:\n",
    "        history_dict[new] = {'loss': [], 'acc': []}\n",
    "        for loop in [1, 2, 3]:\n",
    "            print('Creating Model with the {} {}'.format(new, change))\n",
    "            model_params = params.standard()\n",
    "            model_params['conv_filters'] = new\n",
    "\n",
    "            model = models.basic_cnn(model_params, x_train.shape)\n",
    "\n",
    "            y_pred, metrics = models.fit_model(model, model_params, \n",
    "                                               x_train, y_train, x_test, y_test)\n",
    "\n",
    "            # Adds Data to Trends\n",
    "            history_dict[new]['loss'].append(metrics['loss'])\n",
    "            history_dict[new]['acc'].append(metrics['acc'])\n",
    "\n",
    "        # Calculates Average\n",
    "        history_dict[new]['loss'] = np.mean(history_dict[new]['loss'])\n",
    "        history_dict[new]['acc'] = np.mean(history_dict[new]['acc'])\n",
    "\n",
    "        # Plots Confusion Matrix\n",
    "        classes = {0: 'T-Shirt/top',\n",
    "                   1: 'Trouser',\n",
    "                   2: 'Pullover',\n",
    "                   3: 'Dress',\n",
    "                   4: 'Coat',\n",
    "                   5: 'Sandal',\n",
    "                   6: 'Shirt',\n",
    "                   7: 'Sneaker',\n",
    "                   8: 'Bag',\n",
    "                   9: 'Ankle boot'}\n",
    "        class_values = list(classes.values())\n",
    "        title = \"{} (Loss {} & Acc {})\".format(new, metrics['loss'], metrics['acc'])\n",
    "        conf_png = '{}/{}_{}.png'.format(args.out, new, change)\n",
    "        plot.conf_matrix(y_test, y_pred, class_values, out=conf_png, title=title)\n",
    "\n",
    "    # Plots Accuracy & Loss Trends\n",
    "    trends_png = '{}/{}.png'.format(args.out, change)\n",
    "    plot.dict_trends(history_dict, xlabel=change, out=trends_png)\n",
    "\n",
    "    return x_train, y_train, x_test, y_test, y_pred\n",
    "\n",
    "main(ARGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1: Testing Number of Filters (Output Dimensionality)\n",
    "I wanted to modify the dimensionality of my output filters first, just to get a general idea of where in the world my output filter quantity should be.\n",
    "\n",
    "Let's first keep in mind that this is a 28 x 28 array. If I have a kernel that is 2x2, sliding at a stride of 2 pixels each, this would give me a theoratical ideal output of 14x14. \n",
    "\n",
    "I chose to initially loop between these values of filters, with special emphasis around '14' filters: [4, 8, 12, 14, 16, 20, 24, 28, 32]. The results are attached:\n",
    "\n",
    "![CNN_Filters](TensorData/CNN_FilterTake2/conv_filters.png)\n",
    "\n",
    "Using 14 filters gave me the best results and its confusion matrix is attached below:\n",
    "![CNN_14Filters](TensorData/CNN_FilterTake2/14_conv_filters.png)\n",
    "\n",
    "Something interesting was that when I tested 4 filters, I had runtimes of 48 seconds. But anything greater than that gave me a 50% runtime reduction. On hindsight, this isn't that surprising since there is less data reduction needed at the higher values, but I didn't expect the runtime to drop this dramatically and then plateau even as I added more filters.\n",
    "\n",
    "Recall that the basic neural network had an Accuracy of 0.8821 & Loss of 0.3279. By adding the convolutional layer, albid at a very basic state, we were able to improve those two values slightly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: Testing Kernel Sizes\n",
    "The next parameter to modify was the kernel size which would slide through the image. Because this is a 28x28 image, I wanted to test sizes which presumably would make mathematical sense for such an image. As a result, I tested with [1, 2, 3, 4, 5, 6, 7]. \n",
    "\n",
    "![CNN_Kernel](TensorData/CNN_KernelSize/kernel_size.png)\n",
    "\n",
    "_While the kernel size of 7 isn't plotted below, it was tested, and its results were worse than kernel size 6._\n",
    "\n",
    "The Kernel Size of 2 had the greatest accuracy with lowest loss as noted below:\n",
    "![CNN_2Kernel](TensorData/CNN_KernelSize/2_kernel_size.png)\n",
    "\n",
    "Runtime for each larger kernel size grew linearly. Each epoch was the same amount of seconds as the given kernel size. For example, kernel size 1 had a runtime of 12 Epoch x 1 Size = 12 Seconds and kernel size 2 had a runtime of 12 Epoch x 2 Size = 24 Seconds. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
